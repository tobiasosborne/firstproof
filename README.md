# First Proof

> **WARNING: All proofs in this repository are adversarially generated by an AI system and MUST NOT be trusted.** No proof here should be considered valid unless it has been fully verified in Lean 4 against Mathlib. Until a machine-checkable proof compiles without `sorry` or custom axioms, treat every argument as potentially unsound.

## About

This repository tests the [`af` (Adversarial Proof Framework)](https://github.com/tobiasosborne/vibefeld) tool on the 10 research-level mathematics problems posed in the [*First Proof*](https://1stproof.org) paper by Abouzaid, Blumberg, Hairer, Kileel, Kolda, Nelson, Spielman, Srivastava, Ward, Weinberger, and Williams. The source paper is in [`docs/`](docs/).

### The First Proof Paper

*First Proof* is an experiment to assess the ability of current AI systems to correctly answer research-level mathematics questions. The paper presents 10 problems that arose naturally in the research of the authors, spanning algebraic combinatorics, spectral graph theory, algebraic topology, stochastic analysis, symplectic geometry, representation theory, lattices in Lie groups, tensor analysis, and numerical linear algebra. The answers are known to the authors but were encrypted and posted at [1stproof.org](https://1stproof.org), with public release scheduled for February 13, 2026.

Key features of the benchmark:
- Questions are sampled from the true distribution of problems working mathematicians encounter.
- Answers had never appeared on the internet or in any public forum prior to release.
- Each answer is a proof of roughly five pages or less.
- Preliminary tests by the authors indicate that frontier AI systems (GPT 5.2 Pro, Gemini 3.0 Deepthink) struggle with many of the problems in a single-shot setting.

### What This Repo Does

The `af` tool uses an adversarial prover/verifier protocol to decompose conjectures into proof trees. An AI agent proposes proof steps, and a separate agent attempts to find flaws. **This process is heuristic and fallible** -- the adversarial structure reduces but does not eliminate the risk of unsound arguments.

This repo is a test bed for `af` on hard, real-world mathematical problems whose answers are independently verifiable.

### How the `af` Tool Works

The `af` (Adversarial [proof] Framework) command-line tool manages structured proof trees via a prover/verifier protocol. For the uninitiated mathematician, the workflow is:

1. **Initialization.** A conjecture is registered as the root node of a proof tree (`af init`). Definitions, external references (published theorems), and assumptions are declared.

2. **Decomposition.** The prover decomposes the conjecture into sub-claims (child nodes), each with a precise mathematical statement, inference type, and dependencies on sibling nodes. This creates a tree: proving all children establishes the parent.

3. **Adversarial verification.** A separate verifier agent examines each node and raises *challenges* -- formal objections classified by severity (critical, major, minor). Challenges target the statement, inference, gaps, or dependencies. The prover must resolve each challenge by amending the node or refining it into further children.

4. **Epistemic states.** Each node carries an epistemic label: *pending* (unexamined), *validated* (survived adversarial scrutiny), *refuted* (a challenge disproved it), or *archived* (superseded by a repair). A *taint* flag propagates upward from unresolved challenges.

5. **Iteration.** Multiple prover/verifier waves are run. Refuted nodes are repaired (new children or amended statements), and the tree grows until all leaves are validated or the proof is recognized as incomplete.

The result is not a proof in the formal sense -- it is a structured *proof exploration* that records not only the claimed argument but also the dead ends, counterexamples, and refuted approaches discovered along the way. Each `problemNN/ledger/` directory contains the full history as timestamped JSON entries. LaTeX reports (`report.tex`, `report.pdf`) and handoff documents (`HANDOFF.md`) summarize the state for human readers.

### Why Multi-Shot Exploration Matters

A single-shot proof attempt by an AI system is unlikely to produce a correct proof of a research-level problem. The *First Proof* authors themselves observed that frontier models struggle in this setting. The value of the `af` approach is precisely that it is *not* single-shot:

- **Dead ends are recorded.** When a proof strategy fails, the refutation is preserved. This prevents future attempts (human or AI) from repeating the same mistake. Several problems below have catalogued 5--12 distinct pitfalls that constrain any future proof.
- **Counterexamples are found.** The adversarial verifier has discovered concrete counterexamples to plausible-sounding intermediate claims (e.g., that certain continuum free-probability identities hold at finite n -- they do not).
- **Partial progress is real.** Even where a complete proof is not obtained, validated sub-results (Wick expansion identities, vertex classification lemmas, base-case proofs) represent genuine mathematical content that would survive independent verification.
- **The proof tree is a map.** For a human mathematician approaching one of these problems, the `af` tree provides an annotated map of the mathematical landscape: what has been tried, what works, what fails, and where the genuine difficulties lie.

Were we to have had more token budget and time, it is plausible that some of these proof trees could have been driven to completion -- particularly Problems 1, 3, and 8, which are close. Whether a fully formalized proof (in Lean 4 or otherwise) could have been obtained in this way remains an open and interesting question.

## The 10 Problems

| # | Problem | Author | Field |
|---|---------|--------|-------|
| 1 | Equivalence of the Phi^4_3 measure under smooth shifts | Hairer | Stochastic analysis / QFT |
| 2 | Existence of Whittaker functions for Rankin-Selberg integrals | Nelson | Representation theory |
| 3 | Markov chain with ASEP polynomial stationary distribution | Williams | Algebraic combinatorics |
| 4 | Superadditivity of inverse Phi under free additive convolution | Srivastava / Spielman | Spectral theory |
| 5 | Slice filtration for incomplete transfer systems | Blumberg | Algebraic topology |
| 6 | Existence of large epsilon-light subsets in graphs | Spielman | Spectral graph theory |
| 7 | Lattices with 2-torsion and rationally acyclic universal covers | Weinberger | Lattices in Lie groups |
| 8 | Lagrangian smoothing of polyhedral Lagrangian surfaces | Abouzaid | Symplectic geometry |
| 9 | Algebraic relations on quadrilinear determinantal tensors | Kileel | Tensor analysis |
| 10 | Preconditioned CG for CP decomposition with RKHS constraints | Kolda | Numerical linear algebra |

## Progress Summary (Pre-Solutions Release)

Six of the ten problems were investigated using the `af` tool before the official solutions were released on February 14, 2026. The table below summarizes the state of each `af` proof attempt at the time it was halted. Problems 6, 7, 9, and 10 were not attempted.

| # | Problem | Sessions | Nodes | Validated | Refuted | Challenges | Status |
|---|---------|----------|-------|-----------|---------|------------|--------|
| 1 | Phi^4_3 measure equivalence | 4 | 84 | 21 | 8 (all repaired) | 343 ledger entries | ~85--90% complete |
| 2 | Whittaker functions | 3 | 17 | 5 | 0 | 71 (57 open) | ~29% complete, stuck |
| 3 | ASEP Markov chain | 3 | 9 | 0 | 0 | 97 (all resolved) | Structurally complete, awaiting verification |
| 4 | Finite free Stam inequality | 7+ | 24 | 0 | 0 | Extensive | Open for n >= 4; n <= 3 proved |
| 5 | Slice filtration | 2 | 11 | 0 | 0 | 45 (all open) | Early stage, foundational definition broken |
| 8 | Lagrangian smoothing | 7 | 9 | 2 | 0 | 90 (84 resolved) | ~55--65% confidence in proof |

---

## Post-Mortem: Comparison with Official Solutions

*Added February 14, 2026, after the release of [First Proof Solutions and Comments](docs/FirstProofSolutionsComments.pdf).*

The official solutions were released on February 14, 2026. Each problem folder now contains `solution_official.md` (the correct solution extracted from the authors' PDF) and an updated `report.pdf` (a detailed critical comparison of the `af` attempt against the correct proof). The results are sobering.

### Scorecard

| # | af Answer | Correct Answer | Answer Correct? | Proof Viable? | Key Failure |
|---|-----------|---------------|----------------|--------------|-------------|
| 1 | YES (equivalent) | **NO (mutually singular)** | **WRONG** | No | Confirmation bias; proved toward a false conclusion for 84 nodes |
| 2 | YES (W^circ works) | YES (but different W_0, different proof) | Correct | No | Wrong proof strategy; missed the Godement-Jacquet bridge entirely |
| 3 | YES (t-PushTASEP) | YES (but *interpolation* t-PushTASEP, a new chain) | Correct | No | Wrong Markov chain; reduced to the already-solved non-interpolation problem |
| 4 | TRUE (conjectured) | **TRUE (fully proved)** | Correct | Incomplete | Honest: could not close the proof for n >= 4; missed the Jacobian contraction insight |
| 5 | (no answer) | (construction + characterization) | N/A | No | Vacuous foundational definition; 0% completion |
| 8 | YES (smoothing exists) | **YES** | Correct | Partially | Valid local results; missed the global abstraction (smoothing functions, conormal fibrations) |

**Summary: 1 wrong answer out of 6 attempts. 0 out of 6 produced a correct and complete proof. 4 out of 5 correct answers used the wrong proof strategy.**

### Problem-by-Problem Assessment

Each assessment below has a corresponding detailed analysis in `problemNN/report.pdf`.

#### Problem 1: Phi^4_3 Measure Equivalence -- WRONG ANSWER

**Correct answer: NO.** The measures mu and T_psi\*mu are **mutually singular**, not equivalent. This is proven by Hairer via an event B^gamma (defined through renormalized Wick powers) that has full mu-measure but zero T_psi\*mu-measure. The mechanism is the logarithmically diverging constant c_{N,2} ~ log N: the shifted field produces a term 9(log N)^{-gamma} c_{N,2} <psi, psi> that diverges, ensuring the shifted process almost surely exits B^gamma.

The `af` system spent 4 sessions and 84 nodes building toward the wrong answer. The system fell into precisely the "incorrect heuristic" that Hairer's official solution explicitly warns against: treating the Radon-Nikodym density heuristic as a proof strategy, when in fact the divergent terms in that density are evidence *for* singularity, not obstacles to be overcome.

**What went right despite the wrong answer:** The Wick expansion (Stage B) and renormalization decomposition (Stage C) are mathematically correct and would be useful building blocks in related problems. The system's own refutation of the tilted-measure approach (Session 4) correctly identified that divergent linear tilts break tightness -- this is essentially evidence for the correct answer, but the system interpreted it as a bug to fix rather than a signal to reconsider.

**Root cause:** Confirmation bias. The system committed to YES in Session 1 and never reconsidered. Repeated repair failures (10 refutations) were treated as local bugs rather than evidence that the conjecture itself was false. The `af` framework lacks a mechanism for questioning the root conjecture.

**Risk of false acceptance:** Estimated 30--40%. The remaining gap (Node 1.6.4.3.3, Boue-Dupuis variational identification) was trying to prove something impossible, but the argument was abstract enough that a subtle logical gap might have slipped through adversarial verification.

*Full analysis: [`problem01/report.pdf`](problem01/report.pdf)*

#### Problem 2: Whittaker Functions -- Correct Answer, Wrong Strategy

**Correct answer: YES.** Nelson's proof uses a different Whittaker function (W_0 with compact determinantal support, not the newvector W^circ) and a completely different technique: the Godement-Jacquet functional equation as a duality bridge, which collapses the problem to a single computation with no case splitting.

The `af` system chose W^circ (the newvector) and committed to direct Iwasawa unfolding analysis, requiring three independent proof branches (unramified, supercuspidal, non-supercuspidal ramified). All five systematic issues identified by the `af` verifier are genuine mathematical phenomena -- but they are artifacts of the wrong proof strategy, not obstacles in the correct proof. Nelson's approach sidesteps all of them.

**Risk of false acceptance:** ~5%. The proof was stuck and would never have completed via this strategy.

*Full analysis: [`problem02/report.pdf`](problem02/report.pdf)*

#### Problem 3: ASEP Markov Chain -- Correct Answer, Wrong Chain

**Correct answer: YES.** The correct Markov chain is the *interpolation* t-PushTASEP (Ben Dali-Williams), a genuinely new construction with a novel reinsertion phase governed by site-dependent probabilities. The `af` system proposed the *ordinary* t-PushTASEP and attempted to bridge the gap via a ratio identity f\*_mu/P\*_lambda = f_mu/P_lambda.

This ratio identity is the crux claim (Node 1.6) and is **likely false**: f\*_mu is not proportional to f_mu (they differ by lower-degree terms). The `af` fell into precisely the failure mode identified by the First Proof authors: replacing interpolation polynomials with homogeneous polynomials and solving the already-solved problem.

**Risk of false acceptance:** Estimated 40--50%. This is the most dangerous case. The ratio identity is wrapped in enough algebraic sophistication (Hecke relations, Perron-Frobenius uniqueness) that it could plausibly survive adversarial verification despite being false. All 97 challenges were resolved with 0 remaining open.

*Full analysis: [`problem03/report.pdf`](problem03/report.pdf)*

#### Problem 4: Finite Free Stam Inequality -- Correct Answer, Honest Failure

**Correct answer: TRUE**, proven by Garza Vargas, Srivastava, and Stier. Their proof uses: (1) connecting score vectors to the Jacobian of the root map via heat flow, (2) proving the Jacobian is a contraction on mean-zero vectors using the Bauschke-Guler-Lewis-Sendov theorem on eigenvalue convexity for hyperbolic polynomials, and (3) Blachman's classical argument.

The `af` system correctly conjectured TRUE with 2M+ numerical trials and proved the base cases (n <= 3). It explored three paths (subordination, de Bruijn, EPI), all blocked at hard steps. The key missed insight: variance additivity (which the `af` had already proved) could be differentiated twice to yield a Hessian identity about the Jacobian, and the permutation formula defines a hyperbolic polynomial whose eigenvalue properties close the argument. The `af` had the ingredients but failed to combine them.

This is the most honest failure in the repository: the system correctly identified what it could and could not prove, never claimed more than it had, and its dead-end catalogue is genuinely valuable.

**Risk of false acceptance:** ~0%. The proof was honestly incomplete and the system knew it.

*Full analysis: [`problem04/report.pdf`](problem04/report.pdf)*

#### Problem 5: Slice Filtration -- Vacuous Foundation

**Correct answer:** The official solution by Blumberg, Hill, and Lawson uses *indexed slice categories* -- the correct input to the filtration is an indexing system (collections of admissible finite H-sets), not a family of subgroups.

The `af` system's foundational definition (the "O-admissible family" F_O) collapses to Sub(G) for every transfer system, making the entire construction vacuous. The correct abstraction varies *which composite norms are available*, while the `af` attempted to vary *which subgroups appear*. This is a conceptual error that a domain expert in equivariant homotopy theory would likely have caught immediately.

**Risk of false acceptance:** ~0%. The proof was 0% complete with 45 open challenges.

*Full analysis: [`problem05/report.pdf`](problem05/report.pdf)*

#### Problem 8: Lagrangian Smoothing -- Correct Answer, Partial Overlap

**Correct answer: YES**, proven by Abouzaid via smoothing functions (an intrinsic linear space on the polyhedral surface), conormal fibrations (a global family of Lagrangian planes), and contractibility arguments that avoid all coordinate computations.

The `af` system's vertex classification (Node 1.2) and edge smoothing (Node 1.4) are consistent with the official solution -- they contain the same mathematical content in different language. However, the critical local-to-global step (Node 1.5) fails: the `af` never discovered that the smoothing condition is *linear* (smoothing functions form an affine space), which is what makes partition-of-unity arguments work cleanly. The `af`'s two-zone construction approach is repairable but would require extensive coordinate computations.

**Risk of false acceptance:** 25--35%. The proof direction is correct but the local-to-global gap could have passed adversarial verification without the coordinate compatibility being fully checked.

*Full analysis: [`problem08/report.pdf`](problem08/report.pdf)*

---

### Patterns Across Problems

Several failure modes recur:

1. **Reducing novel problems to known ones.** Problems 2 and 3 both show the `af` system (and LLMs in general) gravitating toward known results rather than constructing genuinely new mathematics. For Problem 3, this meant proposing the wrong Markov chain. For Problem 2, it meant choosing a known-but-wrong Whittaker function.

2. **Confirmation bias.** Problem 1 is the clearest case: the system committed to YES and interpreted every refutation as a local bug rather than evidence for the opposite answer. The `af` framework has no mechanism for questioning the root conjecture.

3. **Missing global abstractions.** Problems 4 and 8 show that the system can handle local computations (base cases, vertex classification, edge smoothing) but struggles to discover the *right global framework* (Jacobian contraction via hyperbolic polynomial theory, smoothing functions via conormal fibrations). These are creative mathematical acts.

4. **The "template trap."** Problem 4 shows the system organizing its proof around continuum analogues (free probability, Shlyakhtenko-Tao) when the correct proof requires genuinely finite-dimensional tools (hyperbolic polynomials, Bauschke et al.). Many continuum identities break at finite n.

5. **Local error detection, global direction blindness.** The adversarial verifier is excellent at catching formula errors, scope violations, circular reasoning, and fabricated citations. It is weak at catching wrong proof strategies, false intermediate claims embedded in correct-looking algebra, and the fundamental question of whether the conjecture is even true.

### Would the af Have Produced Wrong Proofs?

No `af` proof was fully completed. Each contained admitted gaps. The question is: if pushed to completion, would any have been incorrectly accepted?

- **Problems 4 and 5:** No risk. One was honestly incomplete, the other caught at the foundation.
- **Problem 2:** Very low risk. The proof was stuck at genuine hard obstacles and would never have completed.
- **Problem 8:** Moderate risk (25--35%). The proof direction is correct but a quantitative gap in the global assembly could have slipped through.
- **Problem 1:** Significant risk (30--40%). The system was building toward a false conclusion, and the remaining variational argument was abstract enough to potentially fool the verifier.
- **Problem 3:** Highest risk (40--50%). A false ratio identity, wrapped in sophisticated algebra with all challenges resolved, is exactly the kind of claim that could survive adversarial verification.

**The adversarial framework provides strong protection against local errors but incomplete protection against globally wrong proofs.** The most dangerous failure mode is a plausible-sounding but false intermediate claim embedded in otherwise-correct mathematics -- the verifier catches the easy errors but can be fooled by the hard ones.

---

## Note from the Author (Pre-Solutions Release)

*Tobias J. Osborne, February 12, 2026*

I am stopping this experiment here. I am not a domain expert in any of the ten problem areas covered by *First Proof*. Over the course of this investigation I have noticed that this matters enormously: without domain knowledge, I cannot guide the AI agents toward promising proof directions, assess whether a proposed argument is on the right track, or distinguish a genuine insight from a plausible-sounding dead end.

In this sense, this repository is an excellent test of *full automation* -- what happens when you point an adversarial prover/verifier system at hard problems with no human mathematical guidance. The answer, honestly, is mixed. The `af` tool does find real errors (10 refutations in Problem 1 alone, including fabricated citations and false tightness claims). It does discover genuine dead ends and counterexamples. It does produce structured partial progress that would be useful to a domain expert picking up the problem.

But it also produces arguments that I cannot evaluate. For Problem 4, I cannot tell whether the Herglotz coupling lemma is a deep insight or a reformulation that is "as hard as the original conjecture in different language" (the HANDOFF document itself warns of this). For Problem 5, the foundational definition turned out to be vacuous -- something a domain expert in equivariant homotopy theory would likely have caught before three sessions of wasted effort.

By contrast, in my own research area (quantum information theory), the domain knowledge I bring is powerfully accelerated by the `af` tool. I can recognize when a proof direction is promising, redirect the agents away from dead ends early, and provide the feedback pressure that keeps the exploration productive. Outside my area of expertise, I cannot provide that guidance, and I may be creating total junk without knowing it.

The lesson is clear: the `af` tool is a *force multiplier* for domain experts, not a replacement for domain expertise. Full automation on research-level mathematics remains out of reach -- not because the tool cannot find errors or generate arguments, but because *recognizing which arguments are worth pursuing* still requires a mathematician who understands the landscape.

## Note from the Author (Post-Solutions Release)

*Tobias J. Osborne, February 14, 2026*

The official solutions confirm, with painful clarity, that my decision to stop was the right one.

**The headline result: 1 wrong answer, 0 correct complete proofs, and 4 out of 5 correct answers arrived at via the wrong proof strategy.** Problem 1 -- the proof I was most confident in, at "85--90% complete" -- was proving the wrong thing entirely. The answer is NO (mutual singularity), not YES (equivalence). I had no way to know this as a non-expert. The system's own refutations contained evidence for the correct answer, but neither I nor the system recognized it as such.

Problem 3 is perhaps more unsettling. The answer is correct (YES), all 97 challenges were resolved, and the system assessed itself as "structurally complete, awaiting verification." Had I run one more verification wave and the ratio identity survived (which I now estimate at 40--50% probability), I would have had a "complete" proof of the right answer built on a false intermediate claim. I would have had no way to detect this.

### Conclusions

1. **One-shot proofs of research-level mathematics are not viable.** The First Proof authors' own experiments with GPT 5.2 Pro and Gemini 3.0 Deep Think confirm this. Common failure modes include hallucinated citations, solved-problem substitution, and premises that contradict conclusions.

2. **The `af` adversarial framework reduces certain failure modes but does not yet fully automate a proof.** It catches local errors effectively (formula mistakes, scope violations, circular reasoning, fabricated references). It does not reliably catch wrong proof strategies, false intermediate claims wrapped in correct algebra, or -- critically -- wrong answers to the root conjecture. Of the 6 problems attempted, the framework would have stalled honestly on 3 (Problems 2, 4, 5), produced a correct-direction-but-incomplete proof on 1 (Problem 8), and was at material risk of accepting a wrong proof on 2 (Problems 1, 3).

3. **Domain expert guidance is essential.** This was my intuition when I stopped the experiment on February 12, and the post-mortem confirms it decisively. A domain expert would likely have: (a) caught Problem 1's wrong answer by recognizing the divergent constant as evidence for singularity; (b) redirected Problem 2 toward the Godement-Jacquet bridge; (c) recognized that Problem 3 needed a new Markov chain, not a ratio identity; (d) pointed Problem 4 toward hyperbolic polynomial theory; (e) caught Problem 5's vacuous definition immediately. The `af` tool with expert guidance would be substantially more powerful than the `af` tool alone.

4. **The proof trees have value as mathematical maps, not as proofs.** The dead-end catalogues, refutation histories, and pitfall lists in each problem folder represent genuine mathematical content. For Problems 1 and 4 in particular, the catalogues of what *doesn't* work constrain future proof attempts and would save a domain expert significant time. The `af` output is best understood as structured reconnaissance, not claimed proof.

5. **Formal verification (Lean 4 / Mathlib) is the only path to trust.** Nothing in this repository should be treated as correct mathematics. The post-mortem reinforces this in the strongest possible terms: the proof I was most confident in had the wrong answer.

## Repository Structure

```
docs/              Source paper, official solutions PDF
problem01/         84-node proof tree, comparison report, 4 sessions â€” WRONG ANSWER
problem02/         17-node proof tree, comparison report, 3 sessions
problem03/         9-node proof tree, comparison report, 3 sessions
problem04/         24-node proof tree, comparison report, extensive computation
problem05/         11-node proof tree, comparison report, 2 sessions
problem06/         Official solution only (not attempted with af)
problem07/         Official solution only (not attempted with af)
problem08/         9-node proof tree, comparison report, 7 sessions
problem09/         Official solution only (not attempted with af)
problem10/         Official solution only (not attempted with af)
```

Each `problemNN/` folder contains:
- `problem.md` -- LLM-readable problem statement
- `solution_official.md` -- correct solution extracted from the authors' PDF (all 10 problems)
- `report.tex` / `report.pdf` -- detailed critical comparison of `af` attempt vs correct solution (problems 1--5, 8)
- `ledger/` -- `af` proof tree (timestamped JSON entries recording all claims, challenges, amendments)
- `HANDOFF.md` -- session handoff document with priorities and pitfalls
- `externals/` -- references to published theorems used in the proof
- `defs/` -- formal definitions registered with `af`

## Trust Model

**Do not cite, rely on, or reproduce any proof from this repository as correct mathematics.**

The only path to trust is:
1. Formalize the proof in Lean 4 with Mathlib.
2. Compile with zero `sorry` statements and no custom axioms.
3. Run `lake build` successfully.

Everything else -- natural-language arguments, proof sketches, `af` node judgments -- is **unverified conjecture** regardless of how convincing it may appear. The post-mortem analysis demonstrates concretely that even proofs assessed at "85--90% complete" can be entirely wrong.

## License

[Apache License 2.0](LICENSE)
